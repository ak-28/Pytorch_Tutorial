{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torchviz import make_dot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient tracking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Autograd = Autodifferentiation + Dynamic Graph\n",
    "\n",
    "\n",
    "# Autograd in PyTorch is responsible for:\n",
    "\n",
    "# Automatically building a computational graph (as you do operations on tensors)\n",
    "\n",
    "# Automatically computing gradients via the chain rule\n",
    "\n",
    "# Storing gradients in each tensorâ€™s .grad attribute"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ex1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a leaf tensor\n",
    "x = torch.tensor([2.0, 3.0], requires_grad=True)\n",
    "y = torch.tensor([4.0, 5.0], requires_grad=True)\n",
    "\n",
    "z = x**2 + y\n",
    "s = z.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(22., grad_fn=<SumBackward0>)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 8., 14.], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([4., 6.])\n",
      "tensor([1., 1.])\n"
     ]
    }
   ],
   "source": [
    "s.backward()\n",
    "print(x.grad)\n",
    "print(y.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'graph.png'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "make_dot(s).render(\"graph\", format=\"png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ex2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = torch.randn(1, requires_grad=True)\n",
    "b = torch.randn(1, requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.linspace(0,1,100)\n",
    "y = 2*x + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.1\n",
    "for epoch in range(100):\n",
    "    pred = w * x + b\n",
    "    loss = ((pred - y)**2).mean()\n",
    "    loss.backward()\n",
    "    with torch.no_grad():\n",
    "        w -= lr * w.grad\n",
    "        b -= lr * b.grad\n",
    "    w.grad.zero_()\n",
    "    b.grad.zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AddBackward0 at 0x7f25f08ec9a0>"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred.grad_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "x.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## torch.autograd.grad\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What Is torch.autograd.grad?\n",
    "# torch.autograd.grad is a PyTorch function that computes and returns the gradients of tensors with respect to other tensors. Unlike the typical .backward() method, which accumulates gradients into the .grad attribute of leaf tensors, torch.autograd.grad simply returns the gradients as output without modifying any .grad fields.\n",
    "\n",
    "# Why Use torch.autograd.grad?\n",
    "# Functionality: When you want to compute gradients for arbitrary tensors without accumulating them.\n",
    "\n",
    "# Flexibility: Useful for custom optimization routines, meta-learning, or inspecting gradients of intermediate computations.\n",
    "\n",
    "# Control: Allows you to compute higher-order derivatives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.autograd.grad(\n",
    "#     outputs,\n",
    "#     inputs,\n",
    "#     grad_outputs=None,\n",
    "#     retain_graph=False,\n",
    "#     create_graph=False,\n",
    "#     only_inputs=True,\n",
    "#     allow_unused=False\n",
    "# )\n",
    "# outputs: Tensor(s) whose gradients will be computed.\n",
    "\n",
    "# inputs: Tensor(s) with respect to which gradients are computed.\n",
    "\n",
    "# grad_outputs: Optional. Specifies the \"vector\" for which to compute the directional derivative (important for non-scalar outputs).\n",
    "\n",
    "# retain_graph: Whether to retain the computation graph for additional backward passes.\n",
    "\n",
    "# create_graph: If True, computes gradients for higher-order derivatives.\n",
    "\n",
    "# allow_unused: If True, allows missing gradients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(14.)\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor(2.0, requires_grad=True)\n",
    "y = x ** 3 + 2 * x\n",
    "\n",
    "# Compute dy/dx using autograd.grad\n",
    "(grads,) = torch.autograd.grad(y, x)\n",
    "print(grads)  # Output: tensor(14.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_103761/486760323.py:1: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at /pytorch/build/aten/src/ATen/core/TensorBody.h:489.)\n",
      "  y.grad\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
